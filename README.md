# Проектирование и обучение нейронных сетей 
Бланк билетов 2025-2026

✅# Билет 1
<img width="1130" height="451" alt="image" src="https://github.com/user-attachments/assets/f9c48634-2413-49a6-992b-66345a1cddfd" />

**Ответ на вопрос 1**

**Понятие о математическом нейроне. Биологический прототип. Искусственный (математический) нейрон. Использование нейронной сети.**

**Биологический прототип.**  
Математический нейрон является упрощенной моделью биологического нейрона — основной структурной единицы нервной системы. Биологический нейрон состоит из тела (сомы), дендритов (принимают сигналы) и аксона (передает сигналы). Нейроны обмениваются электрохимическими сигналами через синапсы. При превышении суммарного возбуждения порогового значения нейрон генерирует импульс.

**Искусственный (математический) нейрон.**  
Математический нейрон формализует работу биологического нейрона. Он имеет:
- Входы \( x_1, x_2, ..., x_n \), соответствующие дендритам.
- Веса \( w_1, w_2, ..., w_n \), моделирующие силу синаптических связей.
- Сумматор, вычисляющий взвешенную сумму входов:  
<img width="202" height="105" alt="image" src="https://github.com/user-attachments/assets/c10dcd4c-159e-456e-81e0-4ac89782e197" />

- Пороговую функцию активации (например, функцию Хевисайда), которая сравнивает сумму \( S \) с порогом \( \theta \):
<img width="306" height="126" alt="image" src="https://github.com/user-attachments/assets/d8d9fe05-edc1-435c-ab32-dccdf01620b6" />

  Это соответствует переходу нейрона в возбужденное состояние при достаточной суммарной стимуляции.

**Использование нейронной сети.**  
Нейронные сети, состоящие из множества математических нейронов, используются для решения задач классификации образов, распознавания образов, прогнозирования и других задач, связанных с обработкой данных. Они моделируют простые свойства биологических нейронов, такие как суммирование входных сигналов и пороговая активация, но не копируют биологические процессы детально.

Простота математического нейрона позволяет легко его реализовывать и комбинировать в сложные структуры (сети), которые демонстрируют способность к обучению и обобщению на основе данных.

**Ответ на вопрос 2**

**Рекуррентные сети. Вентильные рекуррентные нейронные сети. Долгая краткосрочная память.**

**Рекуррентные сети (РНС)** — это класс нейронных сетей, предназначенных для обработки последовательностей данных, где порядок элементов важен (речь, текст, временные ряды). Их ключевая особенность — наличие обратных связей, позволяющих сохранять информацию о предыдущих состояниях сети. Это делает их пригодными для задач, где контекст и история влияют на текущий результат.

**Долгая краткосрочная память (LSTM)** — это особый тип рекуррентной сети, разработанный для решения проблемы исчезающего градиента и улавливания долгосрочных зависимостей в данных. LSTM включает в себя **вентильные механизмы**:
- **Входной вентиль** — решает, какую новую информацию записать в состояние ячейки.
- **Вентиль забывания** — определяет, какую информацию из состояния ячейки следует удалить.
- **Выходной вентиль** — управляет тем, какая информация из состояния ячейки будет использована для формирования выходного сигнала.

Благодаря этим вентилям LSTM может избирательно сохранять или забывать информацию на длительных промежутках времени, что делает её эффективной для задач машинного перевода, распознавания речи, анализа текстов и других последовательностных задач.

**Вентильные рекуррентные нейронные сети (GRU)** — это упрощённая версия LSTM, в которой объединены вентиль забывания и входной вентиль в один **вентиль обновления**. GRU имеет меньше параметров, чем LSTM, и часто показывает сопоставимую производительность, обучаясь быстрее.

Таким образом, рекуррентные сети, особенно их вентильные варианты (LSTM и GRU), являются мощным инструментом для работы с последовательными данными, где требуется учёт контекста и долгосрочных зависимостей.

**Ответ на вопрос 3**

**Архитектуры сверточных сетей. Сети ResNet.**

Сверточные нейронные сети (СНС) — это глубокие нейронные сети, специально разработанные для обработки данных с сеточной структурой, таких как изображения. Их архитектура включает последовательность сверточных слоёв, слоёв подвыборки (пулинга) и полносвязных слоёв.

**Основные компоненты архитектуры СНС:**
1.  **Свёрточные слои** – применяют фильтры (ядра) к входным данным для извлечения локальных признаков (например, границ, текстур). Используется **разделение параметров** (один фильтр применяется ко всему изображению) и **разреженная связность** (каждый нейрон связан только с локальной областью предыдущего слоя).
2.  **Слои подвыборки (пулинга)** – уменьшают пространственные размеры карт признаков, сохраняя наиболее важную информацию, повышая инвариантность к малым сдвигам и сокращая объём вычислений.
3.  **Полносвязные слои** – располагаются в конце сети и выполняют классификацию на основе извлечённых признаков.

**Проблема исчезающего градиента в очень глубоких сетях:**  
С увеличением глубины сети (количества слоёв) процесс обучения становится сложнее из-за проблемы исчезающего градиента, когда градиенты, распространяемые обратно, становятся чрезвычайно малы, и веса нижних слоёв почти не обновляются. Это ограничивает глубину обучаемых сетей.

**Сети ResNet (Residual Networks):**  
Для решения проблемы исчезающего градиента были предложены сети **ResNet** (сети с остаточными связями). Их ключевая инновация — использование **остаточных блоков (residual blocks)**. Вместо обучения прямой функции отображения \( H(x) \) бло́к обучает **остаточную функцию** \( F(x) = H(x) - x \), а итоговое отображение получается как \( H(x) = F(x) + x \).

**Структура остаточного блока:**  
Входные данные \( x \) пропускаются по **прямой связи (skip connection)** и суммируются с результатом преобразования \( F(x) \), полученного после нескольких свёрточных слоёв. Это позволяет градиенту свободно протекать через эти связи, даже если преобразование \( F(x) \) близко к нулю, что облегчает обучение чрезвычайно глубоких сетей (например, ResNet-152 с 152 слоями).

Таким образом, архитектура ResNet, основанная на остаточных связях, стала прорывом в области глубокого обучения, позволив эффективно обучать очень глубокие свёрточные сети и достигать высоких результатов в задачах компьютерного зрения, таких как классификация, обнаружение и сегментация объектов.

✅# Билет 2

<img width="1116" height="445" alt="image" src="https://github.com/user-attachments/assets/927c69db-9bb5-4999-b721-0ea12fa51508" />

**Ответ на вопрос 1**

**Математический нейрон Мак1Каллока – Питтса. Схематичное изображение. Активационная (пороговая) функция. Пример расчета с порогом логическое «И»**

Математический нейрон Мак-Каллока – Питтса представляет собой формальную модель нейрона с несколькими входами и одним выходом. Каждому входу соответствует вес, а выход определяется на основе пороговой функции активации.

**Схематичное изображение**:
Нейрон включает:
- Входные сигналы \(x_1, x_2, ..., x_n\).
- Веса \(w_1, w_2, ..., w_n\).
- Сумматор, вычисляющий взвешенную сумму:
<img width="215" height="119" alt="image" src="https://github.com/user-attachments/assets/79d98e11-f71e-4fcc-8fee-aa97a5996cbf" />

- Пороговую функцию активации (функцию Хевисайда), которая сравнивает сумму \(S\) с порогом \(T\):
<img width="357" height="125" alt="image" src="https://github.com/user-attachments/assets/e10aac11-b947-4b61-922e-bce59f9cd441" />


**Пример расчета логического «И»**:
Для реализации логического «И» с двумя входами \(x_1\) и \(x_2\) (принимающих значения 0 или 1) используются веса \(w_1 = 1\), \(w_2 = 1\) и порог \(T = 2\).

Расчеты:
1. \(x_1 = 0, x_2 = 0 \rightarrow S = 0 \cdot 1 + 0 \cdot 1 = 0; \quad y = 0\)
2. \(x_1 = 0, x_2 = 1 \rightarrow S = 0 \cdot 1 + 1 \cdot 1 = 1; \quad y = 0\)
3. \(x_1 = 1, x_2 = 0 \rightarrow S = 1 \cdot 1 + 0 \cdot 1 = 1; \quad y = 0\)
4. \(x_1 = 1, x_2 = 1 \rightarrow S = 1 \cdot 1 + 1 \cdot 1 = 2; \quad y = 1\)

Результат соответствует таблице истинности логического «И»: выход равен 1 только при обоих единичных входах.

**Ответ на вопрос 2**

**Рекуррентные сети. Двунаправленные рекуррентные нейронные сети. Блоки с утечками.**

**Рекуррентные сети (РНС)** — это нейронные сети, имеющие обратные связи, позволяющие сохранять информацию о предыдущих состояниях. Они применяются для обработки последовательностей данных (речь, текст, временные ряды), где порядок элементов важен. Примеры: сеть Элмана (Simple RNN), LSTM (Long Short-Term Memory), GRU (Gated Recurrent Unit). Обучение РНС часто проводится методом обратного распространения во времени (Backpropagation Through Time, BPTT).

**Двунаправленные рекуррентные нейронные сети (Bi‑RNN)** используются, когда для предсказания в момент времени \(t\) важна информация не только из прошлого, но и из будущего. Bi‑RNN состоят из двух независимых РНС: одна обрабатывает последовательность в прямом направлении (от начала к концу), другая — в обратном (от конца к началу). Выходы обеих сетей на каждом шаге объединяются (например, конкатенируются или суммируются), что позволяет учитывать контекст со всех сторон. Такие сети эффективны в задачах распознавания речи, машинного перевода, анализа текста.

**Блоки с утечками (Leaky Units)** — это способ улучшить способность РНС запоминать долгосрочные зависимости. Вместо полного обновления скрытого состояния на каждом шаге используется линейная комбинация старого и нового состояний:
<img width="508" height="80" alt="image" src="https://github.com/user-attachments/assets/9764fb66-3bc3-4735-9342-a87071f8a322" />

где \(\alpha\) — параметр «утечки» (близкий к 1). Это позволяет информации сохраняться на много шагов, аналогично экспоненциальному скользящему среднему. Блоки с утечками являются упрощённой альтернативой более сложным механизмам вроде LSTM и помогают бороться с проблемой затухания градиента.

**Ответ на вопрос 3**

**Архитектуры сверточных сетей. Архитектура Inception. Общая схема сети GoogLeNet**

**Архитектуры сверточных сетей** строятся на основе последовательного чередования сверточных слоёв, слоёв пулинга (объединения) и полносвязных слоёв. Сверточные слои извлекают локальные признаки (границы, текстуры), а пулинг-слои уменьшают пространственные размеры, повышая инвариантность к малым сдвигам и деформациям. Глубокие свёрточные сети (такие как VGG, ResNet) позволяют выделять иерархические признаки: от простых на начальных слоях до сложных, семантических — на последующих.

**Архитектура Inception** (также известная как **GoogLeNet**) была предложена для повышения эффективности и глубины сети без чрезмерного роста вычислительной сложности. Её ключевая идея — использование **Inception-модулей**, которые параллельно применяют несколько типов свёрток (1×1, 3×3, 5×5) и операцию пулинга к одному и тому же входу, а затем объединяют их результаты. Это позволяет сети одновременно извлекать признаки разного масштаба и уровня абстракции.

**Общая схема сети GoogLeNet** включает:
1. Начальные свёрточные слои для первичной обработки изображения.
2. Последовательность **Inception-модулей**, каждый из которых состоит из параллельных ветвей:
   - Свёртка 1×1 (для уменьшения размерности и снижения вычислений).
   - Свёртка 3×3.
   - Свёртка 5×5.
   - Максимальный пулинг 3×3 с последующей свёрткой 1×1.
   Все выходы ветвей объединяются по глубине (конкатенация).
3. Вспомогательные классификаторы на промежуточных слоях для борьбы с исчезающим градиентом и улучшения обучения.
4. Глобальный средний пулинг вместо полносвязных слоёв в конце, что уменьшает число параметров.
5. Финальный классификационный слой с функцией Softmax.

Таким образом, GoogLeNet демонстрирует, как можно эффективно комбинировать операции разного масштаба в рамках одного модуля, создавая глубокие и мощные сети для задач компьютерного зрения.

✅# Билет 3

<img width="1129" height="452" alt="image" src="https://github.com/user-attachments/assets/0b55c307-4e1b-4464-87fd-3d6304f7f6e5" />

**Ответ на вопрос 1**

**Математический нейрон Мак-Каллока – Питтса. Схематичное изображение. Активационная (пороговая) функция. Пример расчета с порогом логическое «ИЛИ»**

Математический нейрон Мак-Каллока – Питтса — это формальная модель нейрона, состоящая из нескольких входов, весов, сумматора и пороговой функции активации.

**Схематичное изображение**:
- Входы: \( x_1, x_2, ..., x_n \)
- Веса: \( w_1, w_2, ..., w_n \)
- Сумматор вычисляет взвешенную сумму:
<img width="202" height="113" alt="image" src="https://github.com/user-attachments/assets/624a4315-07c2-4f5d-8d27-cc82fda609bf" />

- Пороговая функция активации (функция Хевисайда) определяет выход:
<img width="337" height="127" alt="image" src="https://github.com/user-attachments/assets/886008b1-18ec-4fca-a658-ed6cb0eb73c9" />

  где \( T \) — порог чувствительности нейрона.

**Пример расчета логического «ИЛИ»**:
Для реализации логического «ИЛИ» с двумя входами \( x_1 \) и \( x_2 \) (принимающих значения 0 или 1) используются веса \( w_1 = 1 \), \( w_2 = 1 \) и порог \( T = 1 \).

Расчеты:
1. \( x_1 = 0, x_2 = 0 \rightarrow S = 0 \cdot 1 + 0 \cdot 1 = 0; \quad y = 0 \)
2. \( x_1 = 0, x_2 = 1 \rightarrow S = 0 \cdot 1 + 1 \cdot 1 = 1; \quad y = 1 \)
3. \( x_1 = 1, x_2 = 0 \rightarrow S = 1 \cdot 1 + 0 \cdot 1 = 1; \quad y = 1 \)
4. \( x_1 = 1, x_2 = 1 \rightarrow S = 1 \cdot 1 + 1 \cdot 1 = 2; \quad y = 1 \)

Результат соответствует таблице истинности логического «ИЛИ»: выход равен 1, если хотя бы один вход равен 1.

**Ответ на вопрос 2**

**Рекуррентные сети. Метод обучения рекуррентной нейронной сети. Функция softmax.**

**Рекуррентные сети (RNN)** — это нейронные сети, в которых присутствуют обратные связи, позволяющие сохранять информацию о предыдущих состояниях. Они предназначены для обработки последовательных данных, где порядок элементов важен (например, речь, текст, временные ряды). Примеры архитектур: сеть Элмана, сеть Хопфилда, LSTM, GRU. RNN могут иметь различные конфигурации: «один ко многим», «многие к одному», «многие ко многим».

**Метод обучения рекуррентной нейронной сети** — **BPTT (Backpropagation Through Time)**.  
Так как RNN работают с последовательностями во времени, стандартный метод обратного распространения ошибки модифицируется. Идея BPTT заключается в «разворачивании» сети во времени: каждый шаг последовательности представляется как отдельный слой в глубокой сети. Градиент ошибки вычисляется для каждого временного шага и распространяется назад по развернутой структуре. Это позволяет учитывать зависимости между удаленными во времени элементами последовательности, но требует больших вычислительных ресурсов и памяти порядка O(τ), где τ — длина последовательности.

**Функция softmax** — это активационная функция, применяемая на выходном слое сети для задач многоклассовой классификации. Она преобразует вектор произвольных значений (логитов) в вектор вероятностей, где сумма всех элементов равна 1. Формула softmax для элемента \( z_i \):

<img width="265" height="102" alt="image" src="https://github.com/user-attachments/assets/5b44db27-6434-4ca8-b145-9a073fa4d12b" />


где \( K \) — количество классов.  
Функция гарантирует, что выходные значения интерпретируются как вероятности принадлежности объекта к каждому классу. Часто используется в сочетании с функцией потерь «перекрестная энтропия» для обучения классификаторов, в том числе в рекуррентных сетях для задач последовательной классификации (например, предсказание следующего слова).

**Ответ на вопрос 3**

**Архитектура сверточных сетей. Схема сети VGG-16.**

**Архитектура сверточных сетей (CNN)** строится на основе последовательного чередования сверточных слоёв (Conv), слоёв подвыборки (пулинга, Pooling) и полносвязных слоёв (FC).  
- **Сверточный слой** применяет набор фильтров (ядер) к входному изображению, извлекая локальные признаки (границы, текстуры и т.д.).  
- **Слой подвыборки (пулинг)** уменьшает пространственные размеры карт признаков, сохраняя наиболее значимую информацию и снижая вычислительную сложность. Часто используется max-pooling.  
- **Полносвязные слои** в конце сети выполняют классификацию на основе извлечённых признаков.

**Сеть VGG-16** — одна из классических глубоких сверточных архитектур, предложенная в 2014 году. Её ключевые особенности:  
- Состоит из **16 весовых слоёв** (13 свёрточных и 3 полносвязных).  
- Использует **маленькие фильтры 3×3** с шагом 1, что позволяет уменьшить количество параметров по сравнению с большими фильтрами, при этом увеличивая глубину сети и её нелинейность.  
- Все скрытые слои используют **функцию активации ReLU**.  
- После нескольких свёрточных слоёв применяется **max-pooling с окном 2×2 и шагом 2**, уменьшающий размер карт признаков вдвое.  
- Архитектура включает **три полносвязных слоя** (два скрытых по 4096 нейронов и выходной слой с 1000 нейронов для классификации ImageNet).  
- Входное изображение имеет размер **224×224×3** (RGB).

**Схема VGG-16** (упрощённо):  
1. Conv3-64 → Conv3-64 → MaxPool  
2. Conv3-128 → Conv3-128 → MaxPool  
3. Conv3-256 → Conv3-256 → Conv3-256 → MaxPool  
4. Conv3-512 → Conv3-512 → Conv3-512 → MaxPool  
5. Conv3-512 → Conv3-512 → Conv3-512 → MaxPool  
6. FC-4096 → FC-4096 → FC-1000 (Softmax)

Основные преимущества VGG-16 — однородность архитектуры и хорошая обобщающая способность, хотя она требует больших вычислительных ресурсов из-за значительного числа параметров.

✅# Билет 4

<img width="1111" height="452" alt="image" src="https://github.com/user-attachments/assets/3c85d9a2-c4e8-45d9-b403-24e66e29587a" />

**Ответ на вопрос 1**

**Математический нейрон Мак-Каллока – Питтса. Схематичное изображение. Активационная (пороговая) функция. Пример расчета со смещением логическое «ИЛИ». Графическая интерпретация**

**Схематичное изображение математического нейрона**  
Математический нейрон состоит из:
- Входных сигналов \(x_1, x_2, ..., x_n\)
- Весов \(w_1, w_2, ..., w_n\)
- Сумматора, вычисляющего взвешенную сумму:  
 <img width="260" height="126" alt="image" src="https://github.com/user-attachments/assets/c577177e-8100-49c8-9df5-9bcb0c02ebe2" />

  где \(b\) – смещение (bias), эквивалентное весу дополнительного входа \(x_0 = 1\).
- Пороговой функции активации (функция Хевисайда), определяющей выход:  
<img width="313" height="115" alt="image" src="https://github.com/user-attachments/assets/21376e10-ff21-429b-8008-c7d3ca285bda" />


**Пример расчета логического «ИЛИ» со смещением**  
Для реализации логического «ИЛИ» с двумя входами \(x_1, x_2\) (значения 0 или 1) выбираются веса \(w_1 = 1, w_2 = 1\) и смещение \(b = -0.5\) (эквивалентно порогу \(T = 0.5\), так как \(b = -T\)).

Расчет взвешенной суммы \(S\) для всех комбинаций входов:
1. \(x_1 = 0, x_2 = 0\):  
   \(S = 0\cdot1 + 0\cdot1 + (-0.5) = -0.5\) → \(y = 0\)
2. \(x_1 = 0, x_2 = 1\):  
   \(S = 0\cdot1 + 1\cdot1 + (-0.5) = 0.5\) → \(y = 1\)
3. \(x_1 = 1, x_2 = 0\):  
   \(S = 1\cdot1 + 0\cdot1 + (-0.5) = 0.5\) → \(y = 1\)
4. \(x_1 = 1, x_2 = 1\):  
   \(S = 1\cdot1 + 1\cdot1 + (-0.5) = 1.5\) → \(y = 1\)

Результат соответствует таблице истинности логического «ИЛИ».

**Графическая интерпретация**  
В пространстве входов \((x_1, x_2)\) разделяющая гиперплоскость задается уравнением:

w_1 x_1 + w_2 x_2 + b = 0

Подставляя \(w_1 = 1, w_2 = 1, b = -0.5\), получаем:

x_1 + x_2 - 0.5 = 0 \quad \Rightarrow \quad x_1 + x_2 = 0.5

Эта прямая разделяет точки \((0,0)\) (класс 0) от точек \((0,1), (1,0), (1,1)\) (класс 1). Все точки, для которых \(x_1 + x_2 \geq 0.5\), классифицируются как 1, что соответствует логическому «ИЛИ».

**Ответ на вопрос 2**

**Рекуррентные сети. Глубокие рекуррентные нейронные сети.**

**Рекуррентные сети** — это класс нейронных сетей, предназначенных для обработки последовательностей данных, где порядок элементов важен. Они обладают внутренней памятью, так как нейроны имеют обратные связи, позволяющие сохранять информацию о предыдущих состояниях. Это делает их пригодными для задач, где контекст и временные зависимости играют ключевую роль: распознавание речи, обработка естественного языка, машинный перевод и т.д.

**Глубокие рекуррентные нейронные сети** представляют собой усложнённые архитектуры рекуррентных сетей, в которых преобразования между слоями или внутри скрытых состояний выполняются многослойно. Это позволяет моделировать более сложные и долгосрочные зависимости. Примеры таких архитектур:

1. **Глубокая рекуррентная сеть с глубоким преобразованием входа (Deep Transition RNN, DT-RNN)** — использует несколько слоёв для преобразования входного сигнала в скрытое состояние на каждом шаге.
2. **Глубокая рекуррентная сеть с глубоким преобразованием выхода (Deep Output RNN, DO-RNN)** — применяет глубокие преобразования между скрытым состоянием и выходом.
3. **Стек рекуррентных сетей** — состоит из нескольких рекуррентных слоёв, где выход одного слоя является входом для следующего, что увеличивает выразительную способность сети.

Для обучения рекуррентных сетей часто используется метод **обратного распространения во времени (Backpropagation Through Time, BPTT)**, который разворачивает сеть во времени и применяет алгоритм обратного распространения ошибки к развёрнутой структуре.

**Ответ на вопрос 3**

**Реализация сверточных сетей. Обучение. Пример выделения признаков на двух сверточных слоях.**

**Реализация и обучение сверточных сетей (СНС)**  
Сверточные нейронные сети реализуются как последовательность слоёв, основными из которых являются:
1. **Сверточные слои** — применяют фильтры (ядра) к входным данным, выделяя локальные признаки. Каждый фильтр скользит по изображению с заданным шагом, вычисляя свёртку.
2. **Слои активации** — добавляют нелинейность (часто ReLU).
3. **Слои пулинга (подвыборки)** — уменьшают размерность карт признаков, сохраняя важную информацию (макс-пулинг, средний пулинг).
4. **Полносвязные слои** — выполняют классификацию на основе выделенных признаков.

**Обучение СНС** происходит с помощью метода обратного распространения ошибки (backpropagation) и градиентного спуска. Функция потерь (например, кросс-энтропия) минимизируется путём обновления весов фильтров и полносвязных слоёв. Для ускорения обучения используются оптимизаторы (Adam, SGD) и методы регуляризации (Dropout, Batch Normalization).

**Пример выделения признаков на двух сверточных слоях**  
Рассмотрим простую архитектуру для обработки изображений:

1. **Первый сверточный слой**:
   - Применяет несколько фильтров небольшого размера (например, 3×3).
   - Выделяет низкоуровневые признаки: границы, углы, текстуры.
   - После свертки применяется активация ReLU для нелинейности.
   - Затем выполняется макс-пулинг для уменьшения размерности.

2. **Второй сверточный слой**:
   - Принимает карты признаков от первого слоя.
   - Использует фильтры большего количества и/или размера.
   - Выделяет более сложные и абстрактные признаки: комбинации границ, простые формы, паттерны.
   - Снова применяется ReLU и пулинг.

Таким образом, первый слой отвечает за обнаружение простых локальных особенностей, а второй — за их комбинирование в более сложные структуры. Это соответствует иерархическому принципу обработки информации в зрительной коре мозга и позволяет сети эффективно распознавать объекты на изображениях.

✅# Билет 5

<img width="1096" height="473" alt="image" src="https://github.com/user-attachments/assets/b1606604-8d5a-4be1-bef8-54700620f145" />

**Ответ на вопрос 1**

**Математический нейрон Мак-Каллока – Питтса. Схематичное изображение. Активационная (пороговая) функция. Пример расчета со смещением логическое «И». Графическая интерпретация.**

**Схематичное изображение математического нейрона:**  
Нейрон включает:
- Входы: \(x_1, x_2, ..., x_n\)  
- Веса: \(w_1, w_2, ..., w_n\)  
- Сумматор вычисляет взвешенную сумму с учётом смещения \(b\):  
<img width="262" height="99" alt="image" src="https://github.com/user-attachments/assets/e9fc773b-2a67-4e50-9217-dfd64cdebe14" />

- Пороговая функция активации (функция Хевисайда) определяет выход:  
<img width="305" height="117" alt="image" src="https://github.com/user-attachments/assets/3c86d07d-6796-48e0-8ecf-142dd1a1e073" />

Здесь смещение \(b\) эквивалентно весу дополнительного входа \(x_0 = 1\) и позволяет сдвигать порог активации.

**Пример расчета логического «И» со смещением:**  
Для двух входов \(x_1, x_2\) (значения 0 или 1) выбираем веса \(w_1 = 1, w_2 = 1\) и смещение \(b = -1.5\) (что соответствует порогу \(T = 1.5\), так как \(b = -T\)).

Рассчитываем взвешенную сумму \(S\) для всех комбинаций входов:

1. \(x_1 = 0, x_2 = 0\):  
   \(S = 0 \cdot 1 + 0 \cdot 1 + (-1.5) = -1.5\) → \(y = 0\)  
2. \(x_1 = 0, x_2 = 1\):  
   \(S = 0 \cdot 1 + 1 \cdot 1 + (-1.5) = -0.5\) → \(y = 0\)  
3. \(x_1 = 1, x_2 = 0\):  
   \(S = 1 \cdot 1 + 0 \cdot 1 + (-1.5) = -0.5\) → \(y = 0\)  
4. \(x_1 = 1, x_2 = 1\):  
   \(S = 1 \cdot 1 + 1 \cdot 1 + (-1.5) = 0.5\) → \(y = 1\)

Результат соответствует таблице истинности логического «И»: выход равен 1 только когда оба входа равны 1.

**Графическая интерпретация:**  
В пространстве входов \((x_1, x_2)\) разделяющая гиперплоскость задаётся уравнением:  

w_1 x_1 + w_2 x_2 + b = 0

Подставляя \(w_1 = 1, w_2 = 1, b = -1.5\), получаем:  

x_1 + x_2 - 1.5 = 0 \quad \Rightarrow \quad x_1 + x_2 = 1.5

Эта прямая разделяет точку \((1,1)\) (класс 1) от остальных точек \((0,0), (0,1), (1,0)\) (класс 0). Только для точки \((1,1)\) выполняется условие \(x_1 + x_2 \geq 1.5\), что соответствует логической функции «И».

**Ответ на вопрос 2**

**Рекуррентные сети. Сеть Элмана. Схема и общий вид нейронной сети Элмана. Модификации схемы работы сети Элмана.**

**Рекуррентные сети** – это класс нейронных сетей, в которых между элементами существуют обратные связи, позволяющие сохранять информацию о предыдущих состояниях. Они используются для обработки последовательных данных (речь, текст, временные ряды), где порядок элементов важен.

**Сеть Элмана** (Simple Recurrent Network, SRN) – одна из базовых архитектур рекуррентных сетей. Она состоит из трёх слоёв:
1. **Входной слой** – принимает входной вектор в момент времени \( t \).
2. **Скрытый слой** – содержит нейроны с обратными связями на самих себя, что позволяет сохранять информацию о предыдущих состояниях сети.
3. **Выходной слой** – формирует выходной сигнал.

**Общий вид сети Элмана** описывается следующими уравнениями:
<img width="418" height="140" alt="image" src="https://github.com/user-attachments/assets/d313aa94-ab83-40bd-b209-7e077e66e8c3" />

где:
- \( h^{(t)} \) – состояние скрытого слоя в момент \( t \),
- \( x^{(t)} \) – входной вектор,
- \( y^{(t)} \) – выходной вектор,
- \( U, W, V \) – матрицы весов,
- \( b, c \) – смещения,
- \( f, g \) – функции активации.

**Схема сети Элмана** включает прямые связи от входа к скрытому слою и от скрытого слоя к выходу, а также рекуррентные связи внутри скрытого слоя, передающие состояние \( h^{(t-1)} \) на следующий шаг.

**Модификации схемы работы сети Элмана**:
1. **Архитектура «много в один» (many-to-one)** – несколько входных шагов преобразуются в один выход (например, классификация последовательности).
2. **Архитектура «один во много» (one-to-many)** – один вход преобразуется в последовательность выходов (например, генерация текста).
3. **Архитектура «много во много» (many-to-many)** – последовательность на входе преобразуется в последовательность на выходе (машинный перевод, обработка речи).

Для обучения сети Элмана используется модифицированный метод обратного распространения ошибки во времени (Backpropagation Through Time, BPTT), который разворачивает рекуррентную сеть в глубокую сеть прямого распространения по временным шагам.

**Ответ на вопрос 3**

**Реализация сверточных сетей. Выбор максимального значения из соседних. Пример max-пулинга. Инвариантность.**

Сверточные нейронные сети (СНС) реализуются с использованием последовательности **сверточных слоёв** и **слоёв подвыборки (пулинга)**.  

**Max-пулинг** — это операция подвыборки, которая уменьшает пространственные размеры карты признаков, сохраняя наиболее значимые признаки. На каждом шаге окно фиксированного размера (например, 2×2) скользит по карте признаков, и в выход записывается **максимальное значение** из элементов, попавших в это окно.

**Пример max-пулинга**:  
Для входной матрицы 4×4:  

<img width="191" height="164" alt="image" src="https://github.com/user-attachments/assets/173eb921-68c3-42c2-99d1-693963438236" />

Применяем окно 2×2 с шагом 2. Получаем:  
- В первом окне (1, 3; 4, 2) максимум = 4  
- Во втором окне (2, 1; 7, 3) максимум = 7  
- В третьем окне (3, 5; 2, 6) максимум = 6  
- В четвёртом окне (1, 2; 4, 8) максимум = 8  

Итоговая матрица 2×2:  
<img width="116" height="99" alt="image" src="https://github.com/user-attachments/assets/c9b828d2-f810-40b1-9455-d8e335390ba2" />


**Инвариантность**, обеспечиваемая max-пулингом:  
- **Пространственная инвариантность к малым сдвигам и искажениям**: поскольку выбирается максимальное значение в окне, сеть становится менее чувствительной к точному расположению признаков.  
- **Уменьшение переобучения**: снижение размерности сокращает количество параметров и вычислительную сложность.  
- **Сохранение наиболее ярко выраженных признаков**: активированные нейроны с наибольшими значениями передаются на следующий слой, что помогает выделять существенные особенности.

Таким образом, max-пулинг является важным компонентом сверточных сетей, обеспечивающим инвариантность к пространственным изменениям и повышающим эффективность обучения.

# Билет 6

<img width="1100" height="452" alt="image" src="https://github.com/user-attachments/assets/92a68a1c-7873-4b62-81aa-37c6bf6765e5" />

**Ответ на вопрос 1**

**Понятие о математическом нейроне.**  
Математический нейрон — это упрощенная модель биологического нейрона, формализующая процесс обработки сигнала. Он состоит из:
- Входных сигналов \(x_1, x_2, ..., x_n\),
- Весов \(w_1, w_2, ..., w_n\),
- Сумматора, вычисляющего взвешенную сумму:
<img width="276" height="114" alt="image" src="https://github.com/user-attachments/assets/c131c574-0184-470e-ad6f-184878a69ad4" />

  где \(b\) — смещение,
- Активационной функции, преобразующей сумму \(S\) в выходной сигнал \(y\).

**Активационная функция** определяет нелинейное преобразование, позволяющее нейрону решать сложные задачи. Её выбор влияет на способность сети к обучению.

**Сигмоидальная функция (логистическая функция):**  
<img width="233" height="96" alt="image" src="https://github.com/user-attachments/assets/bdd53ee7-c2dc-4286-8cd0-11049e76db12" />

Она «сжимает» вход в диапазон \((0, 1)\), что удобно для интерпретации выхода как вероятности. Производная выражается через саму функцию: \(\sigma'(x) = \sigma(x)(1 - \sigma(x))\), что упрощает обратное распространение ошибки.

**Гиперболический тангенс:**  
<img width="297" height="103" alt="image" src="https://github.com/user-attachments/assets/835a1d57-edaa-46e7-9a8b-b8470161912a" />

Принимает значения в диапазоне \((-1, 1)\), центрируя данные, что часто ускоряет сходимость обучения.

**Линейная функция активации:**  

f(x) = cx

Пропорциональна входу, позволяет получать непрерывный спектр значений. Однако её производная постоянна, что затрудняет обучение в глубоких сетях, так как композиция линейных функций остаётся линейной.

**Полулинейный элемент (ReLU — Rectified Linear Unit):**  
<img width="247" height="66" alt="image" src="https://github.com/user-attachments/assets/6537f1d1-3f6c-4822-b3b8-f8a0f9619778" />

Возвращает \(x\), если \(x > 0\), и 0 в противном случае. ReLU нелинейна, вычислительно эффективна, способствует разреженной активации, но может страдать от проблемы «умирающих нейронов» при нулевом градиенте для отрицательных входов.

**Ответ на вопрос 3**

**Сверточные сети. Операция свертки. Пример двумерной свертки. Эффекты границ, дополнение и шаг свертки.**

**Сверточные нейронные сети (СНС)** — это специальный вид нейронных сетей, предназначенных для обработки данных с сеточной структурой, таких как изображения. Они вдохновлены организацией зрительной коры мозга и обладают ключевыми свойствами: **инвариантность к переносу**, **разреженная связность** и **разделение параметров**.

**Операция свертки** — это линейное преобразование входных данных с помощью **ядра** (фильтра). Для двумерного случая (изображение) она вычисляется как взвешенная сумма пикселей в локальной области:

<img width="432" height="126" alt="image" src="https://github.com/user-attachments/assets/17f5f771-c80b-42af-bf22-3079e831c464" />


где:
- \(x\) — входная карта признаков (изображение),
- \(W\) — ядро свертки размера \((2d+1) \times (2d+1)\),
- \(y_{i,j}\) — выходное значение в позиции \((i,j)\).

**Пример двумерной свертки**:  
Если входное изображение имеет размер \(5 \times 5\), а ядро — \(3 \times 3\), то выходная карта признаков будет размером \(3 \times 3\) (без дополнения). Каждый элемент выходной карты вычисляется как сумма произведений элементов ядра на соответствующий фрагмент входного изображения.

**Эффекты границ, дополнение и шаг свертки**:
- **Эффекты границ**: при свертке без дополнения размер выходной карты уменьшается. Например, для входа \(m \times m\) и ядра \(k \times k\) выход будет \((m-k+1) \times (m-k+1)\).
- **Дополнение (padding)**: чтобы сохранить размер выходной карты, вход дополняется нулями по краям. Например, для ядра \(3 \times 3\) добавляется по одному пикселю с каждой стороны.
- **Шаг свертки (stride)**: определяет, на сколько пикселей смещается ядро при каждом шаге. Шаг \(s=1\) означает последовательное перемещение; шаг \(s>1\) уменьшает размер выходной карты и повышает вычислительную эффективность, но снижает разрешение.

Таким образом, сверточные сети эффективно извлекают иерархические признаки из изображений, а управление дополнением и шагом позволяет контролировать размерность и детализацию выходных данных.

# Билет 7

<img width="1100" height="433" alt="image" src="https://github.com/user-attachments/assets/8187765f-d55b-45be-b34e-1a488b64867e" />

**Ответ на вопрос 1**

**Однослойные искусственные нейронные сети. Обучение. Методы обучения.**

**Однослойные искусственные нейронные сети** состоят из одного слоя математических нейронов, каждый из которых непосредственно соединён с входными сигналами. Выходы нейронов являются выходом сети. Такие сети способны решать только линейно разделимые задачи (например, реализация логических функций «И», «ИЛИ», «НЕ»).

**Обучение нейронных сетей** — процесс настройки весов и смещений нейронов с целью минимизации ошибки между выходом сети и целевыми значениями.

**Методы обучения однослойных сетей:**

1. **Метод коррекции ошибки (правило Хебба в модификации):**  
   Веса обновляются пропорционально произведению входного сигнала и разности между желаемым и фактическим выходом:
  <img width="302" height="60" alt="image" src="https://github.com/user-attachments/assets/d3ce78f5-9f3d-4d7b-9b81-60932a2a9670" />

   где \(\eta\) — скорость обучения, \(d\) — желаемый выход, \(y\) — фактический выход, \(x_j\) — входной сигнал.

2. **Персептронное правило (правило Розенблатта):**  
   Обновление весов происходит только при ошибочной классификации:
   <img width="307" height="54" alt="image" src="https://github.com/user-attachments/assets/8b888c47-9e62-4a02-af63-2918263ea440" />

   где \(d\) и \(y\) принимают значения 0 или 1. Правило гарантирует сходимость, если задача линейно разделима.

3. **Метод наименьших квадратов (адаптивный линейный элемент — ADALINE):**  
   Минимизируется квадратичная функция ошибки. Обновление весов выполняется по градиентному спуску:
   <img width="287" height="65" alt="image" src="https://github.com/user-attachments/assets/f894069f-6579-4bdd-91d6-4225e923a367" />

   где \(d\) и \(y\) — непрерывные значения. Это позволяет обучаться на данных с шумом.

4. **Дельта-правило (правило Уидроу — Хоффа):**  
   Частный случай метода наименьших квадратов для линейной активационной функции. Используется для адаптивной фильтрации и прогнозирования.

**Общий алгоритм обучения:**
1. Инициализировать веса малыми случайными значениями.
2. Подать на вход обучающий пример, вычислить выход сети.
3. Вычислить ошибку: \(e = d - y\).
4. Обновить веса: \(w_j^{\text{нов}} = w_j^{\text{стар}} + \Delta w_j\).
5. Повторять шаги 2–4 для всех примеров до сходимости (ошибка становится меньше заданного порога или достигнуто максимальное число эпох).

**Особенности:**
- Однослойные сети не могут решать нелинейно разделимые задачи (например, «исключающее ИЛИ»).
- Обучение проводится только на одном слое весов (между входами и выходами).
- Методы основаны на итеративной коррекции весов с использованием локальной информации об ошибке.

**Ответ на вопрос 2**

**Рекуррентные сети. Сети Хопфилда. Структура сети Хопфилда.**

**Рекуррентные сети** — это класс нейронных сетей, в которых между нейронами существуют обратные связи, что позволяет сети сохранять информацию о предыдущих состояниях. Они применяются для обработки последовательных данных, где порядок элементов важен (распознавание речи, текста, временные ряды).

**Сеть Хопфилда** — это тип рекуррентной нейронной сети, предложенный Дж. Хопфилдом. Она является ассоциативной памятью: способна сохранять образы (векторы) и восстанавливать их по неполным или искаженным входным данным.

**Структура сети Хопфилда**:
1. **Архитектура**: сеть состоит из одного слоя нейронов, каждый из которых связан со всеми остальными, кроме самого себя. Связи симметричны: \( w_{ij} = w_{ji} \).
2. **Нейроны**: бинарные (состояние +1 или -1) или с непрерывной активацией.
3. **Обучение**: веса рассчитываются по правилу Хебба на основе обучающих образцов:
   \[
   w_{ij} = \sum_{s=1}^{p} x_i^{(s)} x_j^{(s)} \quad (i \neq j),
   \]
   где \( p \) — число образцов, \( x_i^{(s)} \) — значение \( i \)-го компонента \( s \)-го образа.
4. **Функционирование**: сеть работает итеративно. На каждом шаге случайно выбирается нейрон, и его состояние обновляется по правилу:
   \[
   S_i = \sum_{j \neq i} w_{ij} x_j, \quad x_i^{\text{нов}} = \text{sign}(S_i).
   \]
   Процесс продолжается до достижения стабильного состояния (аттрактора), которое соответствует одному из запомненных образов.

**Назначение**: сеть Хопфилда используется для ассоциативного recall, решения задач оптимизации (например, задачи коммивояжёра) и как модель автоассоциативной памяти.

<img width="1102" height="473" alt="image" src="https://github.com/user-attachments/assets/07c61cbb-a57a-49e2-9edd-8c38a6662b09" />

**Ответ на вопрос 2**

**Рекуррентные сети. Сети Хопфилда. Алгоритм формирования матрицы синаптических весов.**

**Рекуррентные сети** – это нейронные сети, в которых существуют обратные связи, позволяющие передавать информацию от выходов нейронов обратно на входы (в том числе на входы самих себя). Это позволяет сети сохранять состояние (память) о предыдущих входных данных и использовать его при обработке последующих. Такие сети применяются для задач обработки последовательностей: распознавание речи, машинный перевод, анализ временных рядов.

**Сети Хопфилда** – частный случай рекуррентной сети, предложенный Джоном Хопфилдом. Это однослойная сеть с симметричными весами и обратными связями между всеми нейронами. Каждый нейрон связан со всеми остальными, кроме самого себя (или иногда с самим собой). Сеть функционирует как ассоциативная память: она способна восстановить запомненный образ по его искаженному или неполному варианту.

**Алгоритм формирования матрицы синаптических весов для сети Хопфилда** (правило Хебба для ортогональных образов):

1. Пусть заданы \( p \) бинарных образцов (векторов) \( \mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(p)} \), где каждый вектор имеет размерность \( n \) и его элементы принимают значения \( +1 \) или \( -1 \).

2. Матрица синаптических весов \( W \) размерности \( n \times n \) вычисляется по формуле:

<img width="510" height="133" alt="image" src="https://github.com/user-attachments/assets/4ae18801-71ab-4456-a962-23bf1a633cb7" />


где \( w_{ij} \) – вес связи от нейрона \( j \) к нейрону \( i \), \( x_i^{(k)} \) – \( i \)-й элемент \( k \)-го образца.

3. Матрица \( W \) является симметричной (\( w_{ij} = w_{ji} \)) и имеет нули на главной диагонали.


<img width="1101" height="455" alt="image" src="https://github.com/user-attachments/assets/c208dbde-c099-4044-855e-50e6ee7c072e" />

**Ответ на вопрос 2**

**Рекуррентные сети. Сеть Хемминга. Структура сети Хемминга.**

**Рекуррентные сети** — это нейронные сети, имеющие обратные связи, что позволяет им сохранять информацию о предыдущих состояниях и обрабатывать последовательности данных, где порядок элементов важен (например, речь, текст).

**Сеть Хемминга** — это трёхслойная рекуррентная структура, предложенная Р. Липпманом, которая является развитием сети Хопфилда. Она позиционируется как специализированное гетероассоциативное запоминающее устройство. Основная идея состоит в минимизации расстояния Хемминга между тестовым вектором, подаваемым на вход сети, и векторами обучающих выборок, закодированными в структуре сети.

**Структура сети Хемминга** включает:
1. **Первый слой**: однонаправленное распространение сигналов от входа к выходу с фиксированными весами.
2. **Второй слой (MAXNET)**: состоит из нейронов, связанных обратными связями по принципу «каждый с каждым». Нейроны этого слоя функционируют в режиме WTA (Winner Takes All — победитель получает все), где активируется только один нейрон с максимальной активацией. Веса между нейронами отрицательные (обычно \(-\epsilon\)), а связь нейрона с самим собой положительная (\(+1\)).
3. **Третий слой**: выходной однонаправленный слой, формирующий выходной вектор, в котором только один нейрон имеет выходное значение, равное 1, а остальные — 0.

Сеть используется для задач классификации образов, где необходимо найти наиболее близкий сохранённый образ к входному вектору на основе расстояния Хемминга.

<img width="1098" height="422" alt="image" src="https://github.com/user-attachments/assets/e66ac2dd-2ad4-47db-9d2d-ec5c71d800a6" />

<img width="1096" height="445" alt="image" src="https://github.com/user-attachments/assets/a4e09a52-11cf-466a-ac36-7ab4145dca84" />

**Ответ на вопрос 3**

**Сверточные сети. Биологическая мотивация. Примеры тензоров. Четырехмерный тензор.**

**Сверточные нейронные сети (СНС)** — это специальный класс нейронных сетей, предназначенных для эффективной обработки данных с сеточной структурой, таких как изображения. Их архитектура вдохновлена организацией зрительной коры головного мозга.

**Биологическая мотивация**  
Зрительная кора состоит из иерархически организованных зон (V1, V2, V3, V4, V5, V6, V7), каждая из которых отвечает за извлечение определённых признаков из изображения:
- **V1** — выделяет локальные признаки (границы, ориентацию, пространственную частоту).
- **V2** — обобщает локальные признаки, добавляет бинокулярное зрение.
- **V3** — распознаёт цвет и текстуры.
- **V4** — определяет геометрические фигуры и очертания.
- **V5** — анализирует движение.
- **V6, V7** — обобщают информацию, распознают сложные объекты (например, лица).

Эта иерархическая обработка, где низкие уровни извлекают простые признаки, а высокие — сложные и абстрактные, легла в основу архитектуры сверточных сетей.

**Тензоры в глубоком обучении**  
Тензор — это многомерный массив чисел, обобщающий понятия скаляра, вектора и матрицы. Примеры тензоров:
- **Скаляр** — тензор нулевого ранга (одно число).
- **Вектор** — тензор первого ранга (одномерный массив).
- **Матрица** — тензор второго ранга (двумерный массив).
- **Трёхмерный тензор** — массив матриц (например, набор изображений в оттенках серого).
- **Четырёхмерный тензор** — массив трёхмерных тензоров.

**Четырёхмерный тензор**  
В контексте обработки изображений четырёхмерный тензор используется для представления **пакета (batch) изображений**. Его форма зависит от принятого соглашения:
- **«Канал следует последним»** (TensorFlow): `(образцы, высота, ширина, каналы)`.
- **«Канал следует первым»** (Theano): `(образцы, каналы, высота, ширина)`.

Пример: пакет из 128 цветных изображений размером 256×256 пикселей представляется тензором формы `(128, 256, 256, 3)` в TensorFlow или `(128, 3, 256, 256)` в Theano. Для чёрно-белых изображений количество каналов равно 1.

Таким образом, сверточные сети, биологически мотивированные устройством зрительной системы, эффективно обрабатывают многомерные данные (тензоры), извлекая иерархические признаки из изображений.

<img width="1110" height="453" alt="image" src="https://github.com/user-attachments/assets/78b3b2a7-7245-4664-a5a8-a85ac32f5f8e" />

<img width="1094" height="439" alt="image" src="https://github.com/user-attachments/assets/a20ee19b-34f5-435a-8aba-4ada429c6b48" />

<img width="1095" height="433" alt="image" src="https://github.com/user-attachments/assets/14159a62-7411-404a-acef-19b50fd140d1" />

<img width="1096" height="435" alt="image" src="https://github.com/user-attachments/assets/91e37e32-dc77-49f8-9850-7bee456b5a95" />

<img width="1096" height="461" alt="image" src="https://github.com/user-attachments/assets/626e18be-5727-4543-88a0-30a9e05c4183" />

<img width="1097" height="451" alt="image" src="https://github.com/user-attachments/assets/b3d7e8e9-36bf-4e82-bcab-e277d589ea62" />

<img width="1096" height="403" alt="image" src="https://github.com/user-attachments/assets/9d07c12c-c953-468f-b9a8-3da421d50020" />

<img width="1098" height="377" alt="image" src="https://github.com/user-attachments/assets/9c6f9cf8-cbfe-4012-8945-0955d5b40d1d" />

<img width="1095" height="430" alt="image" src="https://github.com/user-attachments/assets/2a5cc378-06d7-4020-a9f4-68f95f15dcc9" />

<img width="1097" height="456" alt="image" src="https://github.com/user-attachments/assets/8245dd20-ce4c-4af7-951f-3295e1f8ffd3" />

<img width="1094" height="511" alt="image" src="https://github.com/user-attachments/assets/59c2da81-086d-4dcd-b19b-543003a0985f" />

<img width="1098" height="402" alt="image" src="https://github.com/user-attachments/assets/b544df3b-33f8-41e1-ac2b-0c602a722d53" />

<img width="1097" height="404" alt="image" src="https://github.com/user-attachments/assets/bf1e390c-607a-4413-b019-b468b9bc4d3d" />
